[2020-06-01 12:39:41,997] {logging_mixin.py:95} INFO - Sending to executor.
[2020-06-01 12:39:41,998] {logging_mixin.py:95} INFO - [2020-06-01 12:39:41,997] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'bq_data', 'bq_write_to_test', '2020-06-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/bq_data.py']
[2020-06-01 12:39:41,998] {logging_mixin.py:95} INFO - [2020-06-01 12:39:41,998] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'bq_data', 'bq_write_to_test', '2020-06-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/bq_data.py']
[2020-06-01 12:39:43,097] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: bq_data.bq_write_to_test 2020-06-01T00:00:00+00:00 [None]>
[2020-06-01 12:39:43,101] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: bq_data.bq_write_to_test 2020-06-01T00:00:00+00:00 [None]>
[2020-06-01 12:39:43,101] {__init__.py:1353} INFO - 
--------------------------------------------------------------------------------
[2020-06-01 12:39:43,101] {__init__.py:1354} INFO - Starting attempt 1 of 2
[2020-06-01 12:39:43,101] {__init__.py:1355} INFO - 
--------------------------------------------------------------------------------
[2020-06-01 12:39:43,107] {__init__.py:1374} INFO - Executing <Task(BigQueryOperator): bq_write_to_test> on 2020-06-01T00:00:00+00:00
[2020-06-01 12:39:43,107] {base_task_runner.py:119} INFO - Running: ['airflow', 'run', 'bq_data', 'bq_write_to_test', '2020-06-01T00:00:00+00:00', '--job_id', '25', '--raw', '-sd', 'DAGS_FOLDER/bq_data.py', '--cfg_path', '/var/folders/y6/nhshp1rd3kj1kwtclmvq1nfsrtp362/T/tmpooxuk251']
[2020-06-01 12:39:43,758] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test [2020-06-01 12:39:43,757] {__init__.py:51} INFO - Using executor SequentialExecutor
[2020-06-01 12:39:43,984] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test [2020-06-01 12:39:43,984] {__init__.py:305} INFO - Filling up the DagBag from /Users/rebecca.vickery/Documents/Github/airflow_sandbox/airflow/dags/bq_data.py
[2020-06-01 12:39:44,273] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test [2020-06-01 12:39:44,273] {cli.py:517} INFO - Running <TaskInstance: bq_data.bq_write_to_test 2020-06-01T00:00:00+00:00 [running]> on host HX3286T2.local
[2020-06-01 12:39:44,290] {bigquery_operator.py:184} INFO - Executing: 
    #standardSQL
            select date, 
    sum(total_confirmed_cases) as total_confirmed_cases
    from sandbox-278915:test.covid_italy_daily
    group by date
    
[2020-06-01 12:39:44,688] {__init__.py:1580} ERROR - INTERNAL: No default project is specified
Traceback (most recent call last):
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/operators/bigquery_operator.py", line 209, in execute
    cluster_fields=self.cluster_fields,
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 771, in run_query
    default_project_id=self.project_id)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 1992, in _split_tablename
    raise ValueError("INTERNAL: No default project is specified")
ValueError: INTERNAL: No default project is specified
[2020-06-01 12:39:44,690] {__init__.py:1603} INFO - Marking task as UP_FOR_RETRY
[2020-06-01 12:39:44,698] {logging_mixin.py:95} INFO - [2020-06-01 12:39:44,698] {configuration.py:287} WARNING - section/key [smtp/smtp_user] not found in config
[2020-06-01 12:39:44,699] {__init__.py:1615} ERROR - Failed to send email to: ['rebeccavickeryds@gmail.com']
[2020-06-01 12:39:44,699] {__init__.py:1616} ERROR - [Errno 61] Connection refused
Traceback (most recent call last):
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/operators/bigquery_operator.py", line 209, in execute
    cluster_fields=self.cluster_fields,
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 771, in run_query
    default_project_id=self.project_id)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 1992, in _split_tablename
    raise ValueError("INTERNAL: No default project is specified")
ValueError: INTERNAL: No default project is specified

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1605, in handle_failure
    self.email_alert(error)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1829, in email_alert
    send_email(self.task.email, subject, html_content)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/email.py", line 55, in send_email
    mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/email.py", line 101, in send_email_smtp
    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/email.py", line 121, in send_MIME_email
    s = smtplib.SMTP_SSL(SMTP_HOST, SMTP_PORT) if SMTP_SSL else smtplib.SMTP(SMTP_HOST, SMTP_PORT)
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/smtplib.py", line 251, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/smtplib.py", line 336, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/smtplib.py", line 307, in _get_socket
    self.source_address)
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py", line 727, in create_connection
    raise err
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py", line 716, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[2020-06-01 12:39:44,720] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test /Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/operators/bigquery_operator.py:176: DeprecationWarning: Deprecated parameter `bql` used in Task id: bq_write_to_test. Use `sql` parameter instead to pass the sql to be executed. `bql` parameter is deprecated and will be removed in a future version of Airflow.
[2020-06-01 12:39:44,720] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   category=DeprecationWarning)
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test Traceback (most recent call last):
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/bin/airflow", line 32, in <module>
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     args.func(args)
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     return f(*args, **kwargs)
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/bin/cli.py", line 523, in run
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     _run(args, dag, ti)
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/bin/cli.py", line 442, in _run
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     pool=args.pool,
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/db.py", line 73, in wrapper
[2020-06-01 12:39:44,747] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     return func(*args, **kwargs)
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     result = task_copy.execute(context=context)
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/operators/bigquery_operator.py", line 209, in execute
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     cluster_fields=self.cluster_fields,
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 771, in run_query
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     default_project_id=self.project_id)
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 1992, in _split_tablename
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test     raise ValueError("INTERNAL: No default project is specified")
[2020-06-01 12:39:44,748] {base_task_runner.py:101} INFO - Job 25: Subtask bq_write_to_test ValueError: INTERNAL: No default project is specified
[2020-06-01 12:39:48,095] {logging_mixin.py:95} INFO - [2020-06-01 12:39:48,095] {jobs.py:2562} INFO - Task exited with return code 1
