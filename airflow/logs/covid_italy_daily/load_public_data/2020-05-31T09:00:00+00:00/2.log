[2020-06-01 15:20:18,551] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: covid_italy_daily.load_public_data 2020-05-31T09:00:00+00:00 [queued]>
[2020-06-01 15:20:18,557] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: covid_italy_daily.load_public_data 2020-05-31T09:00:00+00:00 [queued]>
[2020-06-01 15:20:18,558] {__init__.py:1353} INFO - 
--------------------------------------------------------------------------------
[2020-06-01 15:20:18,558] {__init__.py:1354} INFO - Starting attempt 2 of 2
[2020-06-01 15:20:18,558] {__init__.py:1355} INFO - 
--------------------------------------------------------------------------------
[2020-06-01 15:20:18,564] {__init__.py:1374} INFO - Executing <Task(BigQueryOperator): load_public_data> on 2020-05-31T09:00:00+00:00
[2020-06-01 15:20:18,564] {base_task_runner.py:119} INFO - Running: ['airflow', 'run', 'covid_italy_daily', 'load_public_data', '2020-05-31T09:00:00+00:00', '--job_id', '39', '--raw', '-sd', 'DAGS_FOLDER/covid_italy_daily.py', '--cfg_path', '/var/folders/y6/nhshp1rd3kj1kwtclmvq1nfsrtp362/T/tmppk1l7nwx']
[2020-06-01 15:20:19,177] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data [2020-06-01 15:20:19,177] {__init__.py:51} INFO - Using executor SequentialExecutor
[2020-06-01 15:20:19,388] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data [2020-06-01 15:20:19,388] {__init__.py:305} INFO - Filling up the DagBag from /Users/rebecca.vickery/Documents/Github/airflow_sandbox/airflow/dags/covid_italy_daily.py
[2020-06-01 15:20:19,759] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data [2020-06-01 15:20:19,759] {cli.py:517} INFO - Running <TaskInstance: covid_italy_daily.load_public_data 2020-05-31T09:00:00+00:00 [running]> on host HX3286T2.local
[2020-06-01 15:20:19,777] {bigquery_operator.py:184} INFO - Executing: 
    #standardSQL
            select date, 
    sum(total_confirmed_cases) as total_confirmed_cases
    from bigquery-public-data.covid19_italy.data_by_region
    where DATE(date) = DATE_SUB('2020-05-31', INTERVAL 1 DAY))
    group by date
    
[2020-06-01 15:20:52,936] {__init__.py:1580} ERROR - <HttpError 404 when requesting https://bigquery.googleapis.com/bigquery/v2/projects/sandbox-278915/jobs?alt=json returned "Not found: Dataset sandbox-278915:airflow">
Traceback (most recent call last):
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/operators/bigquery_operator.py", line 209, in execute
    cluster_fields=self.cluster_fields,
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 849, in run_query
    return self.run_with_configuration(configuration)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 1225, in run_with_configuration
    .insert(projectId=self.project_id, body=job_data) \
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/googleapiclient/_helpers.py", line 134, in positional_wrapper
    return wrapped(*args, **kwargs)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/googleapiclient/http.py", line 907, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: <HttpError 404 when requesting https://bigquery.googleapis.com/bigquery/v2/projects/sandbox-278915/jobs?alt=json returned "Not found: Dataset sandbox-278915:airflow">
[2020-06-01 15:20:52,946] {__init__.py:1609} INFO - All retries failed; marking task as FAILED
[2020-06-01 15:20:52,956] {logging_mixin.py:95} INFO - [2020-06-01 15:20:52,956] {configuration.py:287} WARNING - section/key [smtp/smtp_user] not found in config
[2020-06-01 15:20:52,957] {__init__.py:1615} ERROR - Failed to send email to: ['rebeccavickeryds@gmail.com']
[2020-06-01 15:20:52,957] {__init__.py:1616} ERROR - [Errno 61] Connection refused
Traceback (most recent call last):
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/operators/bigquery_operator.py", line 209, in execute
    cluster_fields=self.cluster_fields,
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 849, in run_query
    return self.run_with_configuration(configuration)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 1225, in run_with_configuration
    .insert(projectId=self.project_id, body=job_data) \
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/googleapiclient/_helpers.py", line 134, in positional_wrapper
    return wrapped(*args, **kwargs)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/googleapiclient/http.py", line 907, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: <HttpError 404 when requesting https://bigquery.googleapis.com/bigquery/v2/projects/sandbox-278915/jobs?alt=json returned "Not found: Dataset sandbox-278915:airflow">

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1613, in handle_failure
    self.email_alert(error)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1829, in email_alert
    send_email(self.task.email, subject, html_content)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/email.py", line 55, in send_email
    mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/email.py", line 101, in send_email_smtp
    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)
  File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/email.py", line 121, in send_MIME_email
    s = smtplib.SMTP_SSL(SMTP_HOST, SMTP_PORT) if SMTP_SSL else smtplib.SMTP(SMTP_HOST, SMTP_PORT)
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/smtplib.py", line 251, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/smtplib.py", line 336, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/smtplib.py", line 307, in _get_socket
    self.source_address)
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py", line 727, in create_connection
    raise err
  File "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py", line 716, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[2020-06-01 15:20:52,973] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data Traceback (most recent call last):
[2020-06-01 15:20:52,973] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/bin/airflow", line 32, in <module>
[2020-06-01 15:20:52,973] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     args.func(args)
[2020-06-01 15:20:52,973] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2020-06-01 15:20:52,973] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     return f(*args, **kwargs)
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/bin/cli.py", line 523, in run
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     _run(args, dag, ti)
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/bin/cli.py", line 442, in _run
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     pool=args.pool,
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/utils/db.py", line 73, in wrapper
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     return func(*args, **kwargs)
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     result = task_copy.execute(context=context)
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/operators/bigquery_operator.py", line 209, in execute
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     cluster_fields=self.cluster_fields,
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 849, in run_query
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     return self.run_with_configuration(configuration)
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/airflow/contrib/hooks/bigquery_hook.py", line 1225, in run_with_configuration
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     .insert(projectId=self.project_id, body=job_data) \
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/googleapiclient/_helpers.py", line 134, in positional_wrapper
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     return wrapped(*args, **kwargs)
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data   File "/Users/rebecca.vickery/.local/share/virtualenvs/airflow_sandbox-x5mYwKUH/lib/python3.7/site-packages/googleapiclient/http.py", line 907, in execute
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data     raise HttpError(resp, content, uri=self.uri)
[2020-06-01 15:20:52,974] {base_task_runner.py:101} INFO - Job 39: Subtask load_public_data googleapiclient.errors.HttpError: <HttpError 404 when requesting https://bigquery.googleapis.com/bigquery/v2/projects/sandbox-278915/jobs?alt=json returned "Not found: Dataset sandbox-278915:airflow">
[2020-06-01 15:20:53,612] {logging_mixin.py:95} INFO - [2020-06-01 15:20:53,611] {jobs.py:2562} INFO - Task exited with return code 1
